{"article_title": "Here\u2019s Why Robots Could Humanize War", "article_keywords": ["battlefield", "ethical", "heres", "robots", "argues", "humanize", "moral", "human", "soldiers", "robot", "war", "damage"], "article_url": "http://www.thefiscaltimes.com/Articles/2014/05/15/Here-s-Why-Robots-Could-Humanize-War", "article_text": "As the Pentagon expands its use of robots on the battlefield and its investments in developing robot technology, a movement to ban the use of autonomous robots on the battlefield is growing. Those who decry the use of robots argue that removing the human element from warfare would remove all moral judgment; robot soldiers would be unfeeling killing machines.\n\nOne researcher, however, believes just the opposite. He argues that robot soldiers would make warfare more ethical, not less.\n\nRonald Arkin, an artificial intelligence expert from Georgia Tech and author of the book, Governing Lethal Behavior in Autonomous Robots, argues in a series of papers that robots can be taught to act morally. He\u2019s presenting his ideas at a United Nations meeting in Geneva this week and sent a 2013 paper, \u201cLethal Autonomous Systems and the Plight of the Non-combatant,\u201d to outline his views.\n\nRelated: We\u2019re One Step Closer to Robots on the Battlefield\n\nArkin says, \u201cIt may be possible to ultimately create intelligent autonomous robotic military systems that are capable of reducing civilian casualties and property damage when compared to the performance of human warfighters.\u201d\n\nIn the paper, Arkin argues that it\u2019s the very inhumanity of robots that allow them to make more humane decisions than their human counterparts. For instance, robots could reduce friendly fire incidents and lower civilian casualties. They could also be programmed to act in what humans would consider a moral way in situations where a human soldier might be tempted to violate the laws of war or ethical and moral codes. He argues that history proves that it\u2019s impossible to prevent soldiers from violating these laws and codes.\n\n\u201cWhile I have the utmost respect for our young men and women warfighters, they are placed into conditions in modern warfare under which no human being was ever designed to function,\u201d he writes. \u201cIn such a context, expecting a strict adherence to the Laws of War \u2026 seems unreasonable and unattainable by a significant number of soldiers.\u201d\n\nAdvantages Over Humans\n\nArkin claims that robots provide an advantage over humans for a host of reasons, including:\n\nThey do not have to worry about self-preservation, and therefore would not have to fire upon targets they simply suspect pose a threat. \u201cThere is no need for a \u2018shoot first, ask-questions later\u2019 approach, but rather a \u2018first-do-no-harm\u2019 strategy can be utilized instead. They can truly assume risk on behalf of the noncombatant,\u201d he writes.\n\nRelated: Robots to Replace Troops on the Battlefield\n\nThey have sensors that are better equipped than a human being to survey the battlefield that allow them to see through the so-called fog of war.\n\nThey could be designed in a way that prevents them from acting out of anger or frustration.\n\nPhysical and mental damage from actions of the battlefield would have no impact on a robot.\n\nThey can process more information than a human before having to use deadly force.\n\nThey could independently monitor the ethical behavior of humans that fight along side it. \u201cThis presence alone might possibly lead to a reduction in human ethical infractions,\u201d Arkin argues.\n\n\n\n\n\nRelated: Killer Robots: If No One Pulls the Trigger, Who\u2019s to Blame?\n\nArkin\u2019s thesis comes at a time when the military is expanding its use of robots on all fronts. They are already used on the battlefield to detect roadside bombs. Private companies and laboratories are also developing robots that can fight fires, haul gear and drag soldiers to safety. It\u2019s only a matter of time before one is weaponized.\n\nAnd it appears as if the military is buying into Arkin\u2019s argument. The Office of Naval Research will give a $7.5 million grant to Tufts, Rensselaer Polytechnic Institute, Yale, Georgetown and Brown researchers to develop a robotic system that can determine right and wrong.\n\nIn his research, Arkin deals only with the moral questions surrounding the use of robots. He does not address the financial issues connected to the job losses that would follow the use of robot soldiers. In theory, they could make human infantry redundant, eliminating hundreds of thousands of jobs for traditional soldiers.\n\nObligation to Use Them?\n\nArkin argues that if science can create weaponized robots that are programmed to always do the right thing under rules of war and recognized moral code, there is an obligation for war planners to use them.\n\n\u201cIf achievable, this would result in a reduction in collateral damage, i.e., noncombatant casualties and damage to civilian property, which translates into saving innocent lives. If achievable this could result in a moral requirement necessitating the use of these systems,\u201d he writes.\n\nTop Reads from The Fiscal Times", "article_metadata": {"og": {"site_name": "The Fiscal Times", "description": "A Georgia Tech professor is making a strong argument for the use of robot soldiers. DOD appears to be listening.", "title": "Here\u2019s Why Robots Could Humanize War", "url": "http://www.thefiscaltimes.com/Articles/2014/05/15/Here-s-Why-Robots-Could-Humanize-War", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/01182013_iRobot_article.jpg", "type": "article"}, "twitter": {"url": "http://www.thefiscaltimes.com/Articles/2014/05/15/Here-s-Why-Robots-Could-Humanize-War", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/01182013_iRobot_article.jpg", "creator": "@davidcfrancis", "card": "summary", "title": "The Fiscal Times"}, "viewport": "initial-scale=1, maximum-scale=1", "description": "A Georgia Tech professor is making a strong argument for the use of robot soldiers. DOD appears to be listening.", "generator": "Drupal 7 (http://drupal.org)"}, "_id": "\"57477af36914bd0286fce9e3\"", "article_summary": "Ronald Arkin, an artificial intelligence expert from Georgia Tech and author of the book, Governing Lethal Behavior in Autonomous Robots, argues in a series of papers that robots can be taught to act morally.\nHe does not address the financial issues connected to the job losses that would follow the use of robot soldiers.\nThose who decry the use of robots argue that removing the human element from warfare would remove all moral judgment; robot soldiers would be unfeeling killing machines.\nArkin argues that if science can create weaponized robots that are programmed to always do the right thing under rules of war and recognized moral code, there is an obligation for war planners to use them.\nAs the Pentagon expands its use of robots on the battlefield and its investments in developing robot technology, a movement to ban the use of autonomous robots on the battlefield is growing."}