"Skeptic" Steven Novella, whom I wrote about here earlier, drinks the Artificial Intelligence Kool-Aid. Novella, with my commentary:

The Future Threat of AI Occasional warnings about artificially intelligent robots taking over the world convulse through the media. There is currently a ripple involving prior interviews with Stephen Hawking and Elon Musk. Their names attract attention, and so the issue will provide a media distraction for a day or two.

Artificial Intelligence isn't here yet, because a truly intelligent Word program wouldn't allow the phrase "convulse through the media" to appear in a document.

In an interview with the BBC, Hawking said: "The development of full artificial intelligence could spell the end of the human race..."

Hawking is right. If humanity ever actually believed that machines came alive and were evil, it would mean that humanity had finally reached a point of apocalyptic stupidity.

In an interview in June with CNBC, Elon Musk said: "I think there's things that are potentially dangerous out there. ...There's been movies about this, like Terminator. There's some scary outcomes and we should try to make sure the outcomes are good, not bad."

The Terminator couldn't even control the California State Legislature. Can he really control the world?

Machines that can think are a staple of science fiction, indicating that there is a fascination with the topic. Most often artificially intelligent machines threaten humanity, such as in Terminator, The Matrix, and Battlestar Galactica.

Novella has read widely on the subject:

In the Dune series humanity is almost wiped out by machines, leading to a ban on any machines that mimic the mind of a person.

Computer sandworms -- now that's really frightening.

Even in Star Wars, where droids are the humble servants of biological creatures, we are warned that if droids could think for themselves "we would all be in trouble."

Novella isn't alone in his distrust of droids. Jar-Jar Binks didn't trust them, either.

...The problem, of course, is that we are too early in the process of designing true AI, a fully self-aware machine intelligence, to predict what will happen when we cross that threshold. It is certainly reasonable to consider the risks. There is general agreement that human level intelligence does not represent any real limit...

Artificial Intelligence hysteria is helping to define the lower limit of human intelligence.

If we have a computer that can think as fast and well as a human in 50 years, then in 100 years we might have a machine that can think 1 million times faster...

Or maybe one billion times faster...

It is only reasonable to consider if such AI might pose a threat to humanity.

Those who are optimistic about AI point to the potential boon they can provide to humanity. They can potentially accelerate research and technological development by orders of magnitude.

AI could be used to design anti-psychotic drugs to treat people who fear the evil plots of their computers.



...This suggests another kind of threat that AI might pose. Science Fiction focuses mainly on AI competing with humanity, enslaving us or wiping us out. AI, however, may also fulfill their role as caretakers of humanity, just too well. They may take a paternalistic approach to this task, protecting us from ourselves, taking away our freedoms to keep us safe and secure. We might become an infantilized species under our robot caretakers.

A computer-generated Affordable Care Act would be terrifying -- "If you like your humanity you can keep your humanity..."

...At this point I don't think we know what will happen. Perhaps at some point every possible outcome will occur to some degree, given enough time. Perhaps it is inevitable that machines will rule the universe, and biology is just a stepping stone. When we finally meet aliens, will they be biological, machines, or a fusion of the two?

Aliens?

...Putting AI in command of weapons also seems like a horrifically bad idea. Humans always have to be in the loop, with their hand on the plug. While these steps may seem obvious, my concern is that competition among nations may motivate some to forgo such safeguards, out of fear that their enemies won't, if nothing else. We may have an AI arms race. International agreements, and a mechanism to enforce them, to avoid such outcomes seems prudent.

The threat is real. Putin is having secret high-level meetings with computers as we speak.

The bigger picture is that humans are developing many types of technologies that contain the potential for serious abuse or just unintended consequences, such as the development of nuclear weapons, biological and chemical weapons, and increasingly vital technological infrastructures. Meanwhile our world is anything but universally enlightened and peaceful. We are making progress, but is it fast enough?

Novella believes that the world is made safer by our preoccupation with evil robots.

Perhaps, as Carl Sagan observed, all civilizations might go through this phase where there is a race between their technological development and their social maturity, with many not surviving. He was referring mostly to nuclear weapons, but there are other threats more subtle and profound.

Sometimes my task of making these guys look ridiculous is just too easy.

To sum up: Dr. Steven Novella, Yale University School of Medicine clinical neurologist, self-styled science-defender and "skeptic," has denied for years that the universe has a cause, denied that living things manifest teleology, and denied that the mind may be something more than meat.

Yet Novella believes that computers are on the verge of waking up and devising evil plans to destroy mankind, and he asserts (matter-of-factly) that when we finally meet the space aliens who rule the universe, they will be mechanical or biological or some kinda hybrid.

I'm laughing too hard. I must stop now.

Image: By Opcnup at English Wikipedia [Public domain], via Wikimedia Commons.