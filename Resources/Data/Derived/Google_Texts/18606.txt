The AlphaGo program devised by DeepMind has beaten a human master of the game of Go and is set to play the world's leading player of the game in March. Nearly two decades ago, IBM's Deep Blue beat world chess champion Gary Kasparov using basically brute force computation. The AlphaGo program is different. In an article in Nature, the artificial intelligence researchers at DeepMind explain how they developed the system. From the abstract:

The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.

An accompanying editorial notes that AlphaCo's play is "intuitive" and that the folks at DeepMind do not know what the AlphaGo system is "thinking" when it makes a move. This observation provokes the editors to speculate about how we humans will have to deal with the advent of artificial intelligences whose workings we don't (and can't) understand:

As shown by its results, the moves that AlphaGo selects are invariably correct. But the interplay of its neural networks means that a human can hardly check its working, or verify its decisions before they are followed through. As the use of deep neural network systems spreads into everyday life — they are already used to analyse and recommend financial transactions — it raises an interesting concept for humans and their relationships with machines. The machine becomes an oracle; its pronouncements have to be believed. When a conventional computer tells an engineer to place a rivet or a weld in a specific place on an aircraft wing, the engineer — if he or she wishes — can lift the machine’s lid and examine the assumptions and calculations inside. That is why the rest of us are happy to fly. Intuitive machines will need more than trust: they will demand faith.

Just as reminder, Hebrews 11:1 declares: "Now faith is the substance of things hoped for, the evidence of things not seen."

Is the age of computational thaumaturgy about to dawn?