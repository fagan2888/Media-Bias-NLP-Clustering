{"article_title": "Our weird robot apocalypse: How paper clips could bring about the end of the world", "article_keywords": ["end", "ai", "intelligence", "superintelligent", "humans", "robot", "artificial", "bostrom", "clips", "bring", "paper", "ais", "human", "world", "weird", "apocalypse"], "article_url": "http://www.salon.com/2014/08/17/our_weird_robot_apocalypse_why_the_rise_of_the_machines_could_be_very_strange/", "article_text": "Topics: AI, Artificial Intelligence, Editor's Picks, Robot Apocalypse, science-fiction, The Future, Innovation News, Technology News, News\n\nNick Bostrom is explaining to me how superintelligent AIs could destroy the human race by producing too many paper clips.\n\nIt\u2019s not a joke. Bostrom, the director of the Future of Humanity Institute at Oxford University, is the author of \u201cSuperintelligence: Paths, Dangers, Strategies,\u201d an exploration of the potentially dire challenges humans could face should AIs ever make the leap from Siri to Skynet. Published in July, the book was compelling enough to spur Elon Musk, the founder and CEO of Tesla, into tweeting out a somber warning:\n\nWorth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes. \u2014 Elon Musk (@elonmusk) August 3, 2014\n\nVia Skype call from his office in Oxford, Bostrom lays out a thought experiment that demonstrates how all our affairs could go awry.\n\nIt doesn\u2019t have to be paper clips. It could be anything. But if you give an artificial intelligence an explicit goal \u2014 like maximizing the number of paper clips in the world \u2014 and that artificial intelligence has gotten smart enough to the point where it is capable of inventing its own super-technologies and building its own manufacturing plants, then, well, be careful what you wish for.\n\n\u201cHow could an AI make sure that there would be as many paper clips as possible?\u201d asks Bostrom. \u201cOne thing it would do is make sure that humans didn\u2019t switch it off, because then there would be fewer paper clips. So it might get rid of humans right away, because they could pose a threat. Also, you would want as many resources as possible, because they could be used to make paper clips. Like, for example, the atoms in human bodies.\u201d\n\nThen Bostrom moves on to even more unsettling scenarios. Suppose you attempted to constrain your budding AIs with goals that seem perfectly safe, like making humans smile, or be happy. What if the AI decided to achieve this goal by \u201ctaking control of the world around us, and 10 paralyzing human facial muscles in the shape of a smile?\u201d Or decided that the best way to maximize human happiness was to stick electrodes in our pleasure centers and \u201cget rid of all the parts of our brain that are not useful for experiencing pleasure.\u201d\n\n\u201cAnd then you end up filling the universe with these vats of brain tissue, in a maximally pleasurable state,\u201d says Bostrom.\n\nAnd if you think that Keanu Reeves has a snowball\u2019s chance in hell of actually outwitting a real Matrix, well, think again.\n\nBut I don\u2019t know. To me, the kind of paper clip doom described by Bostrom doesn\u2019t seem very superintelligent at all. It seems kind of dumb. If we succeed in creating machines that actually become smarter than us \u2014 so much smarter that they redesign themselves to become even more brilliant, setting off what Bostrom calls an \u201cintelligence explosion\u201d that leaves puny humans far behind \u2014 wouldn\u2019t they be smart enough to understand what we meant, instead of taking us literally at our word? Why must doom be inevitable? Why couldn\u2019t our superintelligent spawn be able to grasp the lessons inherent in the existing corpus of human knowledge and help us prosper? Why does it always have to be Skynet that\u2019s not around the corner, instead of utopia?", "article_metadata": {"description": "If you dread a robot revolt, stop worrying about killer computers, and start worrying about ... paper clips?", "title": "Our weird robot apocalypse: How paper clips could bring about the end of the world", "og": {"site_name": "Salon", "description": "If you dread a robot revolt, stop worrying about killer computers, and start worrying about ... paper clips?", "title": "Our weird robot apocalypse: How paper clips could bring about the end of the world", "url": "http://www.salon.com/2014/08/17/our_weird_robot_apocalypse_why_the_rise_of_the_machines_could_be_very_strange/", "image": "http://media.salon.com/2014/08/paper_clip.jpg", "type": "article"}, "twitter": {"description": "If you dread a robot revolt, stop worrying about killer computers, and start worrying about ... paper clips?", "title": "Our weird robot apocalypse: How paper clips could bring about the end of the world", "image": "http://media.salon.com/2014/08/paper_clip.jpg", "creator": "@koxinga21", "site": "@salon", "card": "summary_large_image"}, "author": "Andrew Leonard", "apple-itunes-app": "app-id=549374205, app-argument=http://www.salon.com/", "fb": {"pages": 120680396518, "app_id": 456294607741273}, "keywords": "Salon.com, AI, Artificial Intelligence, Editor's Picks, Robot Apocalypse, science-fiction, The Future", "viewport": "width=device-width, initial-scale=1.0"}, "article_summary": "Also, you would want as many resources as possible, because they could be used to make paper clips.\n\u201cOne thing it would do is make sure that humans didn\u2019t switch it off, because then there would be fewer paper clips.\nIt doesn\u2019t have to be paper clips.\n\u201cHow could an AI make sure that there would be as many paper clips as possible?\u201d asks Bostrom.\nTopics: AI, Artificial Intelligence, Editor's Picks, Robot Apocalypse, science-fiction, The Future, Innovation News, Technology News, NewsNick Bostrom is explaining to me how superintelligent AIs could destroy the human race by producing too many paper clips."}