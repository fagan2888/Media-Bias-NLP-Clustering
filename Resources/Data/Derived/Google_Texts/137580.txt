Topics: AI, Artificial Intelligence, Editor's Picks, Robot Apocalypse, science-fiction, The Future, Innovation News, Technology News, News

Nick Bostrom is explaining to me how superintelligent AIs could destroy the human race by producing too many paper clips.

It’s not a joke. Bostrom, the director of the Future of Humanity Institute at Oxford University, is the author of “Superintelligence: Paths, Dangers, Strategies,” an exploration of the potentially dire challenges humans could face should AIs ever make the leap from Siri to Skynet. Published in July, the book was compelling enough to spur Elon Musk, the founder and CEO of Tesla, into tweeting out a somber warning:

Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes. — Elon Musk (@elonmusk) August 3, 2014

Via Skype call from his office in Oxford, Bostrom lays out a thought experiment that demonstrates how all our affairs could go awry.

It doesn’t have to be paper clips. It could be anything. But if you give an artificial intelligence an explicit goal — like maximizing the number of paper clips in the world — and that artificial intelligence has gotten smart enough to the point where it is capable of inventing its own super-technologies and building its own manufacturing plants, then, well, be careful what you wish for.

“How could an AI make sure that there would be as many paper clips as possible?” asks Bostrom. “One thing it would do is make sure that humans didn’t switch it off, because then there would be fewer paper clips. So it might get rid of humans right away, because they could pose a threat. Also, you would want as many resources as possible, because they could be used to make paper clips. Like, for example, the atoms in human bodies.”

Then Bostrom moves on to even more unsettling scenarios. Suppose you attempted to constrain your budding AIs with goals that seem perfectly safe, like making humans smile, or be happy. What if the AI decided to achieve this goal by “taking control of the world around us, and 10 paralyzing human facial muscles in the shape of a smile?” Or decided that the best way to maximize human happiness was to stick electrodes in our pleasure centers and “get rid of all the parts of our brain that are not useful for experiencing pleasure.”

“And then you end up filling the universe with these vats of brain tissue, in a maximally pleasurable state,” says Bostrom.

And if you think that Keanu Reeves has a snowball’s chance in hell of actually outwitting a real Matrix, well, think again.

But I don’t know. To me, the kind of paper clip doom described by Bostrom doesn’t seem very superintelligent at all. It seems kind of dumb. If we succeed in creating machines that actually become smarter than us — so much smarter that they redesign themselves to become even more brilliant, setting off what Bostrom calls an “intelligence explosion” that leaves puny humans far behind — wouldn’t they be smart enough to understand what we meant, instead of taking us literally at our word? Why must doom be inevitable? Why couldn’t our superintelligent spawn be able to grasp the lessons inherent in the existing corpus of human knowledge and help us prosper? Why does it always have to be Skynet that’s not around the corner, instead of utopia?