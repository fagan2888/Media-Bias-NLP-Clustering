{"article_title": "Computers as Oracles: True Answers We Won't Understand", "article_keywords": ["wont", "search", "neural", "computers", "play", "answers", "alphago", "game", "program", "understand", "oracles", "human", "games", "moves", "true", "networks"], "article_url": "http://reason.com/blog/2016/01/27/computers-as-oracles-true-answers-we-won", "article_text": "The AlphaGo program devised by DeepMind has beaten a human master of the game of Go and is set to play the world's leading player of the game in March. Nearly two decades ago, IBM's Deep Blue beat world chess champion Gary Kasparov using basically brute force computation. The AlphaGo program is different. In an article in Nature, the artificial intelligence researchers at DeepMind explain how they developed the system. From the abstract:\n\nThe game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.\n\nAn accompanying editorial notes that AlphaCo's play is \"intuitive\" and that the folks at DeepMind do not know what the AlphaGo system is \"thinking\" when it makes a move. This observation provokes the editors to speculate about how we humans will have to deal with the advent of artificial intelligences whose workings we don't (and can't) understand:\n\nAs shown by its results, the moves that AlphaGo selects are invariably correct. But the interplay of its neural networks means that a human can hardly check its working, or verify its decisions before they are followed through. As the use of deep neural network systems spreads into everyday life \u2014 they are already used to analyse and recommend financial transactions \u2014 it raises an interesting concept for humans and their relationships with machines. The machine becomes an oracle; its pronouncements have to be believed. When a conventional computer tells an engineer to place a rivet or a weld in a specific place on an aircraft wing, the engineer \u2014 if he or she wishes \u2014 can lift the machine\u2019s lid and examine the assumptions and calculations inside. That is why the rest of us are happy to fly. Intuitive machines will need more than trust: they will demand faith.\n\nJust as reminder, Hebrews 11:1 declares: \"Now faith is the substance of things hoped for, the evidence of things not seen.\"\n\nIs the age of computational thaumaturgy about to dawn?", "article_metadata": {"og": {"site_name": "Reason.com", "description": "Artificial Intelligence and DeepMind", "title": "Computers as Oracles: True Answers We Won't Understand", "url": "http://reason.com/blog/2016/01/27/computers-as-oracles-true-answers-we-won", "image": "https://d1ai9qtk9p41kl.cloudfront.net/assets/db/14539251758924.jpg", "type": "article"}, "twitter": {"image": "https://d1ai9qtk9p41kl.cloudfront.net/assets/db/14539251758924.jpg", "title": "Computers as Oracles: True Answers We Won't Understand", "description": "Artificial Intelligence and DeepMind", "card": "summary_large_image", "site": "@reason"}, "fb": {"pages": 17548474116}, "google-site-verification": "-AmMiilAN-pnjWt369CUWae-VQ-b9wmH4fSGyhbkQxw", "msvalidate.01": "212A68A07785B3080038A845C4EE8905", "article": {"tag": "Science", "published_time": "2016-01-27T21:33:00+00:00", "author": "http://reason.com/people/ronald-bailey"}, "viewport": "width=device-width, initial-scale=1, maximum-scale=2"}, "article_summary": "The AlphaGo program is different.\nThese deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.\nWithout any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play.\nBut the interplay of its neural networks means that a human can hardly check its working, or verify its decisions before they are followed through.\nHere we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves."}