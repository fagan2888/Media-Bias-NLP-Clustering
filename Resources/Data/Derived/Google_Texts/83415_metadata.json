{"article_title": "Three Steps the U.S. Can Take to Stop Killer Robots", "article_keywords": ["wallach", "united", "intelligence", "killer", "stop", "robots", "weapons", "states", "defense", "steps", "human", "military", "autonomous", "secretary"], "article_url": "http://www.thefiscaltimes.com/2016/02/20/Three-Steps-US-Can-Take-Stop-Killer-Robots-0", "article_text": "The United Nations\u2019 effort to ban killer robots will fail, but there are three important steps the United States can take to help slow the rise of lethal autonomous weapons systems, one of the most prominent voices in the robotics debate said this week.\n\nPentagon officials insist they don\u2019t want to allow an autonomous weapon to kill people without a human in the loop, but greater levels of autonomy and artificial intelligence are making their way into more and more pieces of military technology, like in recognizing targets, piloting drones, and driving supply trucks. Defense Department leaders advocate for robotic intelligence and autonomy as thread-reducing (and cost-saving) measures key to securing the United States\u2019 technological advantage over adversaries over the coming decades (the so-called \u2018third offset\u2019 strategy). Defense Secretary Ash Carter, Deputy Defense Secretary Bob Work and former Defense Secretary Chuck Hagel have talked up the importance of artificial intelligence to the military\u2019s future plans.\n\nRelated: Should We Have Laws to Control Robots Before They Control Us?\n\n\u201cWe know we\u2019re going to have to go somewhere with automation,\u201d Air Force Brig. Gen. John Rauch, director of ISR (intelligence, surveillance, and reconnaissance) Capabilities for the Air Force, said at a Tuesday breakfast in Washington sponsored by AFCEA, a technology industry association. Rauch was referring to the rapidly growing demands on human image analysts in the Air Force, especially as additional small drones enter service in the years ahead. \u201cIt\u2019s: \u2018What level of automation is allowed?\u2019 And then when you start talking about munitions, it becomes a whole nother situation.\u201d The Air Force will be coming out with a flight plan for small unmanned aerial systems, or UAS\u2019s, in the next four months, Lt. Gen. Robert Otto, deputy chief of staff for Intelligence, Surveillance and Reconnaissance, said at the meeting.\n\nUN officials working to govern international rules on conventional weapons, however, have been meeting since 2014 on a calls by scientists, academics, human rights activists, and NGOs, to ban autonomous weapon systems, with another meeting scheduled for April.\n\nYale bioethicist Wendell Wallach doused that notion on Sunday. \u201cIt\u2019s going to be hard to put an arms control agreement in place for robotics. The difference between an autonomous weapons system and non-autonomous may be just a difference of a line of code,\u201d he told a packed room on Sunday at the annual meeting of the Association for the Advancement of Science the Lollapalooza of general science, a meeting that draws brilliant brains from across the globe to make major announcements about new discoveries.\n\nThe rapid rise of artificial intelligence is cause for optimism and concern among Defense Department leaders. Deputy Defense Secretary Work, at a Center for New American Security event in December (to which Defense One was a media partner), said, \u201cWe know that China is already investing heavily in robotics and autonomy and the Russian Chief of General Staff [Valery Vasilevich] Gerasimov recently said that the Russian military is preparing to fight on a roboticized battlefield and he said, and I quote, \u2018In the near future, it is possible that a complete roboticized unit will be created capable of independently conducting military operations.\u2019\u201d\n\nRelated: The Robot Revolution Could Wipe Out 5.1 Million Jobs by 2020\n\nWallach implied the automatic arms race to come won\u2019t result in some malevolent evil weapon. The future will, in fact, bring us robot tools sharing the battle space with humans that are far scarier, very stupid, poorly-designed, but incredibly well-armed.\n\n\u201cThe vast preponderance of people in the [U.S.] military \u2026 are very concerned about having robots on battlefields that can respond in milliseconds when the human response time is roughly 250 miliseconds, and often much longer. So we are going down a pretty dangerous road if we allow this to be an arms race between lethal autonomous weapons,\u201d he said.\n\nHere\u2019s what Wallach proposed.\n\n1. An executive order from the president proclaiming that lethal autonomous weapons constitute a violation of existing international humanitarian law. The \u201cexisting\u201d part is key as that means that no one has to draft a new provision. The United States should draw a moral line and call machines making life and death decisions mala en se, or a thing that is bad in and of itself, because they are unpredictable and can\u2019t be fully controlled, and their use would make attribution and affixing responsibility for wrongful death difficult if not impossible. The executive order would stress that designing lethal robotic systems would make the designer responsible for the actions that that robot takes.\n\nRelated: Killer Robots: If No One Pulls the Trigger, Who\u2019s to Blame?\n\n2. Create an oversight and governance coordinating committee for AI. \u201cThe committee should be mandated to favor soft governance solutions (industry standards, professional codes of conduct, etc.) over laws and regulatory agencies in forging adaptive solutions to recognized risks and dangers,\u201d Wallach told Defense One. He said that the committee might include a mix of government, military, business leaders, and public representatives, but the ultimate makeup should be part of a larger discussion.\n\n3. Direct 10 percent of the funding in artificial intelligence to studying, shaping, managing and helping people adapt to the \u201csocietal impacts of intelligent machines.\u201d That doesn\u2019t just mean dedicating part of your research project money to figuring out how not to make your robot deadly. It also refers to figuring out how the artificial intelligence you are designing could throw people out of work, and how to help them adapt to that.\n\nAnother idea that Wallach has floated in the past is embedding theorists and ethicists on AI design teams where they could build machines capable of what Wallach calls \u201coperational morality,\u201d or a type of scaled-down, robot-friendly morale reasoning free from all the philosophical underpinnings and grey-matter clutter that beset human discussions about good and bad. Operational morality would help the robot know when it\u2019s in \u201ca morally significant situation.\u201d In his book A Dangerous Master: How to Keep Technology From Slipping Beyond Our Control Wallach describes it thus: \u201cEngineers already program values and moral behavior into computers and robots\u2026the engineers determine in advance the various situations robots will encounter within tightly constrained contexts. They then program the system to act appropriately in each situation.\u201d\n\nIn those situations where the machine is encountering an unexpected event, the problem becomes more difficult, but not impossible.\n\nOf course, if you talk to members of the military leadership, they say the 2012 directive that then-Deputy Defense Secretary Ash Carter signed forbids automating the kill decision and they aren\u2019t interested in changing it. When it comes to offensive weapons like armed drones, a human must be the ultimate decision-maker.\n\nPhysicist Mark Grubard points out that a directive is different than a moratorium. He argues that the Pentagon is still racing towards very dangerous and very autonomous weapons.\n\nWallach acknowledged Defense Department ambivalence and the importance of the 2012 directive. In the end, he said, it was up to the United States to take a clearer leadership role in keeping death bots off of the battlefield.\n\n\u201cIf that\u2019s really how the secretary of defense feels,\u201d he said, \u201cthen let\u2019s have the United States stop sitting on the sidelines, put a foot forward, and put this international principle in place.\n\nThis article originally appeared on Defense One. Read more Defense One:\n\nStart Preparing for the Collapse of the Saudi Kingdom\n\nKenya Wants Its Own Guantanamo\n\nA Cyberattack Has Paralyzed a Los Angeles Hospital", "article_metadata": {"og": {"site_name": "The Fiscal Times", "description": "The United Nations\u2019 effort to ban killer robots will fail, but there are three importa", "title": "Three Steps the U.S. Can Take to Stop Killer Robots", "url": "http://www.thefiscaltimes.com/2016/02/20/Three-Steps-US-Can-Take-Stop-Killer-Robots-0", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/04022013_terminator_robot_article.jpg", "type": "article"}, "twitter": {"url": "http://www.thefiscaltimes.com/2016/02/20/Three-Steps-US-Can-Take-Stop-Killer-Robots-0", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/04022013_terminator_robot_article.jpg", "card": "summary", "title": "The Fiscal Times"}, "generator": "Drupal 7 (http://drupal.org)", "description": "The United Nations\u2019 effort to ban killer robots will fail, but there are three importa", "viewport": "initial-scale=1, maximum-scale=1"}, "article_summary": "Defense Secretary Ash Carter, Deputy Defense Secretary Bob Work and former Defense Secretary Chuck Hagel have talked up the importance of artificial intelligence to the military\u2019s future plans.\nAn executive order from the president proclaiming that lethal autonomous weapons constitute a violation of existing international humanitarian law.\nThe United Nations\u2019 effort to ban killer robots will fail, but there are three important steps the United States can take to help slow the rise of lethal autonomous weapons systems, one of the most prominent voices in the robotics debate said this week.\nSo we are going down a pretty dangerous road if we allow this to be an arms race between lethal autonomous weapons,\u201d he said.\nIn the end, he said, it was up to the United States to take a clearer leadership role in keeping death bots off of the battlefield."}