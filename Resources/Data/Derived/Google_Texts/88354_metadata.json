{"article_title": "Robots Know Who Wrote That Unsigned Supreme Court Decision", "article_keywords": ["opinions", "supreme", "court", "justices", "decision", "work", "unsigned", "robots", "know", "courts", "writing", "li", "decisions", "wrote"], "article_url": "http://www.theatlantic.com/technology/archive/2016/01/one-step-closer-to-a-robot-supreme-court/424800/", "article_text": "Please consider disabling it for our site, or supporting our work in one of these ways\n\nPredicting the outcome of Supreme Court decisions has long been a favorite parlor game for political scientists, attorneys, and legal-system junkies. People have built statistical models, predictive algorithms, and flow charts, and have used machine learning to try to guess what the justices will decide. Some of these models are reliable. Several of them make accurate predictions around 75 percent of the time. Human prognosticators can be even more impressive than that. One man, deemed by the website FiveThirtyEight to be \u201cthe best Supreme Court predictor in the world,\u201d won FantasySCOTUS\u2014yep, like fantasy football but for the high court\u2014several years in a row and boasts an 80 percent accuracy rate for guessing what the justices will do. But there\u2019s another realm of Supreme Court activity where computers can tease out information that otherwise stays hidden even after a decision is issued: unsigned opinions. Last term, justices issued eight such decisions\u2014meaning, as The New York Times pointed out, more than 10 percent of court\u2019s docket were unsigned.\n\n\u201cPresumably meant to correct errors so glaring that they did not warrant extended consideration, they nonetheless illuminated a trend in the court\u2019s work,\u201d Adam Liptak wrote for the Times last year. \u201cIn most of them, one of two things happened. Prisoners challenging their convictions lost. Or law enforcement officials accused of wrongdoing won.\u201d Unsigned decisions (or, per curiam decisions, as they're known in the legal world) have \u201carguably been abused by courts, by the Supreme Court, and by lower courts,\u201d says William Li, a 2016 computer-science graduate of M.I.T. who has been tracking the high court\u2019s unsigned decisions for years. \u201cIt\u2019s a way of hiding behind a veil of anonymity,\u201d he told me, \u201ca mechanism that kind of removes accountability from them.\u201d So Li and his colleagues\u2014Pablo Azar, David Larochelle, Phil Hill,\n\nJames Cox, Robert Berwick, and Andrew Lo\u2014built an algorithm designed to determine which justice wrote unsigned opinions. (Or which justice\u2019s clerks, as is often the case.) Their work began in 2012, amid rumors that John Roberts, the chief justice, had changed his mind at the last minute about the Affordable Care Act\u2014a move that apparently meant he ended up writing most of the majority opinion after having already written much of the dissent. Li and his colleagues wanted to find out if that theory might be true. They used a combination of statistical data mining and machine learning to glean each justice\u2019s individual writing style, based on years of their signed opinions. The bot works by analyzing a backlog of opinions and plucking out the words, phrases, and sentence structures that characterize a justice\u2019s unique style. The system then assigns a higher weight to those terms, so it knows what to look for when scanning a per curiam decision. Roberts, they learned, uses the word \u201cpertinent\u201d a lot. \u201cHe seems to tend to start sentences with the word \u2018here,\u2019 and end sentences with \u2018the first place\u2019\u2014as in, \u2018in the first place.\u2019\u201d Li said. \u201cBreyer uses, \u2018in respect to.\u2019 For Antonin Scalia, one predictive [word] was \u2018utterly,\u2019 and starting the sentence off with \u2018of course.\u2019 It does seem like there are these kinds of different writing signatures that exist.\u201d", "article_metadata": {"description": "Computer models can determine the authorship of unsigned legal decisions with startling accuracy.", "author": "Adrienne LaFrance", "og": {"site_name": "The Atlantic", "description": "Computer models can determine the authorship of unsigned legal decisions with startling accuracy.", "title": "Robots Could Make the Supreme Court More Transparent", "locale": "en_US", "image": "https://cdn.theatlantic.com/assets/media/img/mt/2016/01/RTX224MY/facebook.jpg?1453302326", "url": "http://www.theatlantic.com/technology/archive/2016/01/one-step-closer-to-a-robot-supreme-court/424800/", "type": "article"}, "twitter": {"domain": "theatlantic.com", "site": "@theatlantic", "card": "summary"}, "ROBOTS": "INDEX, FOLLOW", "p": {"domain_verify": "68e1a0361a557708fefc992f3309ed70"}, "fb": {"admins": "577048155,17301937", "page_id": 29259828486, "app_id": 100770816677686}, "keywords": "The Atlantic, The Atlantic Magazine, TheAtlantic.com, Atlantic, news, opinion, breaking news, analysis, commentary, business, politics, culture, international, science, technology, national and life", "article": {"author": "https://www.facebook.com/adriennelafrance"}, "viewport": "initial-scale=1.0, maximum-scale=1.0, width=device-width, user-scalable=no"}, "article_summary": "But there\u2019s another realm of Supreme Court activity where computers can tease out information that otherwise stays hidden even after a decision is issued: unsigned opinions.\nwho has been tracking the high court\u2019s unsigned decisions for years.\n\u201cPresumably meant to correct errors so glaring that they did not warrant extended consideration, they nonetheless illuminated a trend in the court\u2019s work,\u201d Adam Liptak wrote for the Times last year.\nPlease consider disabling it for our site, or supporting our work in one of these waysPredicting the outcome of Supreme Court decisions has long been a favorite parlor game for political scientists, attorneys, and legal-system junkies.\nLast term, justices issued eight such decisions\u2014meaning, as The New York Times pointed out, more than 10 percent of court\u2019s docket were unsigned."}