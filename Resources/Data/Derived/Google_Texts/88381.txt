But we’ve also been made to understand that subprime lenders and their Wall Street funders didn’t act alone. Instead, they were aided by the avarice of the American people, who were not victims of the crash so much as accomplices in it. Respondents to a Rasmussen poll done during the throes of the crisis overwhelmingly blamed “individuals who borrowed more than they could afford” (54 percent) over Wall Street (25 percent). To this day, the view is widespread and bipartisan: Main Street was an essential cause of the meltdown. The enemy was us.

“It all goes back to the increase in the tolerance for debt,” David Brooks wrote a couple of years ago. The Brookings Institution, meanwhile, has argued that of all the possible crisis narratives, “ ‘everyone was at fault’ comes closest to the truth.” The “wider society” must face the music, it said in a 2009 paper. “People in all types of institutions and as individuals became blasé about risk-taking and leverage.”

Or, as Michael Lewis, our financial-writer laureate, observed: “The tsunami of cheap credit ... was temptation, offering entire societies the chance to reveal aspects of their characters they could not normally afford to indulge.” Reveal themselves, they did, and it wasn’t pretty. Writing for Vanity Fair, Lewis quoted at length Dr. Peter Whybrow, a British neuroscientist at UCLA, who posited that human beings have “the core of the average lizard.” Lewis, running with the analogy, relayed Whybrow’s conclusion, which was that “the succession of financial bubbles, and the amassing of personal and public debt” were “simply an expression of the lizard-brained way of life.”

Is that not the truth?

Actually: No, it’s not. The notion that American consumers share the blame for the mortgage crisis is a lie. And it is one of the most pernicious out there.





Everyone-Is-To-Blame (or EITB, for brevity’s sake) has done much to mute the public outcry essential for sweeping efforts to respond to the financial catastrophe. To the extent that Dodd-Frank fell short of the root-and-branch reform that followed the last great crash in 1929, EITB is to blame. The fact that banks too big to fail before the crisis have been allowed to grow to twice their pre-bubble sizes can be traced to a nagging sense that they didn’t act alone. And if you wonder why, six years after the fact, no significant Wall Street figure has been criminally prosecuted, I would suggest that EITB has muddied perceptions just enough to allow the administration to sidestep the necessary legal mobilization. If everyone is to blame, then criminal indictments of individual executives can be framed as exercises in scapegoating.

Everyone-is-to-blame did its worst damage to the Home Affordable Modification Program, or HAMP, an effort rolled out in the immediate aftermath of the crisis to reduce borrowers’ monthly payments through refinancing or principal write-downs. It was the mere idea of HAMP that set off Rick Santelli on his 2009 rant about “losers’ mortgages” and their “extra bathroom,” sparking the Tea Party revolt. The prospect of helping delinquent borrowers, while others paid theirs on time, unleashed a flood of ressentiment that filled the Congressional Record with denunciations of “irresponsible” actors who “lied” only to wind up in line for “gift equity,” and “tax-payer subsidized windfall.” Wisconsin Representative Jim Sensenbrenner introduced the concept of “happy-go-lucky borrowers” and “cagey borrowers.” Jim Bunning, then Kentucky’s junior senator, felt compelled to warn against helping homeowners “who made bad decisions.” The outpouring tapped into a sentiment powerful enough to silence even some liberals and turned hamp into a political disaster for the Obama administration. Left adrift, the program went from a potential lifeline for borrowers to a fee-machine for servicers and a Kafkaesque nightmare for those it was supposed to help.

As an agent of obfuscation, EITB is a gift that keeps on giving. In October, The Washington Post’s editorial board objected to a $13 billion mortgage-era civil settlement with J.P. Morgan largely because it unfairly singled out the bank, when, in fact, “everyone, from Wall Street to Main Street to Washington, acted on widely held economic beliefs that turned out not to be true.” A forthcoming book by Bob Ivry, a Polk Award–winning investigative reporter for Bloomberg News (and, full disclosure, a friend), eloquently inveighs against big banks and their Washington lackeys, but also includes this assertion: “In the years leading up to the Great Bubble-Burst of 2008, everybody got a chance to cash in. ... If you wanted to buy a place to live, you could get more house than you ever dreamed. You could use your rising home equity for the Disney vacation, the power boat, the fourth bedroom or the college education.”

Such thinking has become the industry standard. In a survey of financial-crisis authors and assorted experts, the overwhelming majority of respondents chose “everybody was at fault” (25 authors) over Wall Street (10) or the government (11).

I haven’t voted yet, so make that 25-11-11. But even as I write this, I worry that it is too late. Like concrete poured into a foundation, EITB is now hardening into a central pillar of the history of the financial crisis. The fallacy risks ensuring that, when the next crash comes, we will be destined to again botch the response. It’s time to tear out the lie and replace it with the truth.

The alleged responsibility that the average American bears for the crisis rests largely on the notion that people knowingly (key word) took out bigger, riskier loans than they could afford—and that they all decided to do it rather suddenly around 2004. Which, see, that’s crazy right there.

Under empirical examination, the myth falls apart completely. Were EITB true, borrowers would have to own at least a third of the blame for the mortgage meltdown, the rest split between Wall Street and the government. The available evidence of borrower culpability comes nowhere near that number.

To say that borrowers don’t deserve equal blame for the crisis doesn’t mean that every individual borrower was innocent. There is no question that “mortgage fraud”—which the FBI defines as misrepresentations relied upon by underwriters, i.e. banks—did account for a percentage of the losses. It’s just that the amount of borrower behavior fitting that definition is vanishingly small. Before it collapsed, for instance, New Century reported that borrowers had failed to make even the first payment on 2.5 percent of its loans. That doesn’t speak well of borrowers, at all. It’s also only 2.5 percent. Zooming out, the Treasury Department reported that so-called “suspicious activity” reported by banks peaked at 137,000 incidents in 2006. But even if every single one of those reports represents actual borrower fraud (spoiler: they don’t), that’s still only about 1 percent of the 14 million mortgages made that year.

One way a borrower can defraud a lender is to pretend to plan on living in the home—because mortgages on primary residences are easier to obtain and carry lower rates—when in fact you’re buying it as an investment or vacation property. We know from lawsuits brought by the conservator for Fannie Mae that the number of owner-occupied houses in the mortgage pool was off by as much as 15 percentage points. Some portion of those houses belonged to people who said they lived in them, but didn’t. That’s definitely something buyers fib about. On the other hand, it often requires a wink from the bank, since residency is easy to check. But most important: Even if the number is the full 15 percent, that’s still well south of EITB.

In 2010, an FBI report drawing on figures from the consultancy Corelogic put total fraudulent mortgages during the peak boom year of 2006 at more than $25 billion. Twenty-five billion dollars is obviously not nothing. But here again, teasing those mortgages out of that year’s crisis-related write-downs of $2.7 trillion from U.S.-originated assets leaves our infamous “cagey” borrowers to blame for only a tiny share of the damage, especially since not all of the fraudulent mortgages were their fault. The ratio looks roughly something like this:

Yes, some of our cab drivers, shoeshine boys, and other fellow citizens tricked a lender into helping them take a flyer on the housing market. But the combined share of the blame for bad mortgages that can be placed on the public sits—and I’m really rounding up here—in the high single digits, and not the much larger, fuzzier numbers in our heads.

The fact is that defrauding a bank that actually cares about the quality of a loan is actually rather difficult, no matter how aggressive or deceitful the borrower. Lenders, on the other hand, can lie with relative ease about all sorts of things, and mountains of evidence show they did so on a widespread basis. For starters, it’s lenders who establish the loan-to-value ratio for a property: how much money the buyer is borrowing versus the house’s estimated worth. Banks didn’t used to let you take out a mortgage too close to the home’s total cost. But play with those numbers and, voilà, a rejected loan application turns into an accepted one. Leading up to the crash, some banks’ representations about loan-to-value ratios were off by as much as 40 percentage points.

Then there was the apparent rampant corruption of appraisals, which also have nothing whatsoever to do with borrowers. Before the bubble popped, appraisers’ groups collected 11,000 signatures on a petition decrying pressure by banks to arrive at “dishonest” or inflated valuations.

And that’s to say nothing of lenders misleading borrowers directly—a practice that the Financial Crisis Inquiry Commission, the Levin-Coburn report, and lawsuits by attorneys general around the country have all found was very much systemic. Mortgage brokers forged borrowers’ signatures and altered documents; Ameriquest (those guys again!) had its own “art department,” as it was known internally, for precisely that function. Oh, and remember those 137,000 instances of “suspicious activity” about possible borrower misdeeds? For the sake of perspective, Citigroup settled a Federal Trade Commission case alleging sales deception that involved two million clients in a single year. That’s what we call wholesale, and it was happening before the mortgage era even really got started.

Today, there’s a big and growing body of documentation about what happened as the financial system became incentivized to sell as many loans as possible on the most burdensome possible terms: Millions—and millions—of borrowers were sold subprime despite qualifying for better.

Perhaps the most astonishing and unappreciated finding comes from The Wall Street Journal, which back in December 2007 published a study of more than $2.5 trillion in subprime loans dating to 2000 (that is to say, most of the subprime loans of the era). The story, by my former colleagues Rick Brooks and Ruth Simon, painted the picture of a world gone upside-down: During the worst years of the frenzy, more than half the subprime loans issued went to borrowers who had credit scores “high enough to often qualify for conventional loans with far better terms.” In 2006, the figure hit 61 percent. Along with its article, the Journal illustrated the alarming trend line with a version of the following graphic:

It goes without saying that no one would voluntarily eschew a prime loan for subprime—subprime is called that for a reason, carrying higher, often escalating rates; pre-payment penalties that “shut the backdoor” by precluding refinancing; and other burdens tacked on for good measure. The Journal concluded that its analysis “raises pointed questions about the practices of major mortgage lenders.” That’s putting it mildly!

Lately, a flying squadron of scholars and lawyers, taking up where journalism left off, has dug further into federal housing data to uncover new mind-melting patterns. One of them, Jacob Faber, a Ph.D. candidate at New York University, last year came up with what may be the second-most astonishing fact about the crisis era: In 2006, the year of the locust, some households earning more than $200,000 annually were more likely to be put in subprime than others earning just $32,000. For those unlucky borrowers, income—the best measure of ability to repay—was not the variable that determined the quality of the loan they received.

Short version: That’s not how a fair market works. But it’s what happened. And the real reason those $200,000-plus borrowers found themselves in subprime leads us to the darkest origins of the lending practices that wrecked the economy—as well as of the troubling way we’ve misallocated accountability.

The idea that the American mortgage borrower went off the deep end has a psychological appeal that is broad and deep. It’s almost reflexive and speaks to a collective misanthropic streak: We know, after all, how people are. For op-ed writers and cocktail-party-goers everywhere, the “we-all-did-it” view has the advantage of projecting a world-weariness that can be taken for sophistication.

This attitude also has a literary pedigree. Cultural theorizing about our inherent weakness goes back to the Bible (see Genesis 2:4-3:24, Adam / Eve), but it was Scottish journalist Charles Mackay who most famously dissected the specific phenomenon of contagious folly in his 1841 classic, Extraordinary Popular Delusions and the Madness of Crowds. Mackay’s work chronicled episodes of mass hysteria—witch hunts, the crusades, alchemy. But most famously, Mackay gave us the parable of the Dutch tulip mania of 1636 to 1637, when flower bulbs briefly became one of the world’s most expensive commodities. A wry and witty stylist, Mackay mixes anecdotes—like one about a sailor who mistook a priceless tulip bulb for an onion to go with his herring breakfast—with mordant observations about human nature. “In reading the history of nations, we find that, like individuals, they have their whims and their peculiarities; their seasons of excitement and recklessness, when they care not what they do,” Mackay’s preface to the 1852 edition begins. “We find that whole communities suddenly fix their minds upon one object, and go mad in its pursuit.”

Mackay has attracted plenty of support from academics over the decades, particularly scholars of social psychology, and in the years since the crash, his work has been much cited as a master theory of what went wrong. People are greedy. What can you do?

There’s just one problem: The accounts that undergird Mackay’s thesis might be wrong. As Andrew Odlyzko, a University of Minnesota mathematician who studies financial panics, puts it, Madness “enjoys extraordinarily high renown in the financial industry and among the press and the public. It also has an extraordinarily low reputation among historians.” Peter Garber, a Brown economist, found in a 1990 paper that the most intense speculation in the Dutch tulip market of the era involved only the rarest bulbs, which had been infected by a certain virus that produced particularly intricate patterns in the flower. After that, the market behaved pretty much the way the market for rare bulbs always behaves. Prices for newly cultivated bulbs were high, then fell over time. In fact, the average decline in prices for the rarest bulbs in the five years after the tulip market crashed was, at most, 32 percent. “Large, but hardly the stuff that legends are made of,” Garber writes.

It’s worth noting here that Mackay himself cited the pivotal role in tulip mania played by bulb brokers, or “jobbers,” as they were known. “Ever on the alert for a new speculation,” Mackay wrote, these professionals resorted to “all the means they so well know how to employ to cause fluctuations in prices.” Tulip jobbers were pikers compared with our twenty-first-century mortgage dealers. Yet contemporary readers seem to miss that part of Mackay’s story—and meanwhile take the rest as gospel. I have attended too many panel discussions during which business reporters and editors—even as they dutifully acknowledge that Wall Street behaved very badly indeed—airily discuss how “we” all profited from the boom. At a breakfast gathering in 2009 at a swank private club on New York’s Upper East Side, a nationally known financial writer claimed he knew a crash was coming when his personal trainer quit to become a real-estate broker. Knowing chuckles all around.

Here’s a new thesis: Much more so than borrowers, it’s the elites who’ve chronicled and shaped beliefs on the bust who truly have deluded themselves. Michael Lewis put it as only he can in a passage, still discussing UCLA’s Professor Whybrow, that describes what appears to be a Sherlock moment. If you plot the geography of American personal debt, Lewis wrote, and then place it atop “the Centers for Disease Control’s color-coded map that illustrates the fantastic rise in rates of obesity across the United States since 1985,” the two would look about the same. “The boom in trading activity in individual stock portfolios; the spread of legalized gambling; the rise of drug and alcohol addiction—it is all of a piece,” Lewis wrote. “Everywhere you turn you see Americans sacrifice their long-term interests for short-term rewards.”

It turns out, Lewis / Whybrow were wrong about those maps. As Reuters blogger Felix Salmon later pointed out, obesity and personal debt are in fact negatively correlated; states (California, New Jersey) with the highest debt had the lowest obesity. But such wild stabs at causation have been very much in fashion. The Washington Post’s Richard Cohen, for instance, once tried to link spiking mortgage defaults to the increasing popularity of tattoos. Something about this particular mass disaster—the forced uprooting of ten million Americans, more than the Dust Bowl migration—has made pedigreed observers leap to firm conclusions about their fellow citizens’ honesty and common sense based on the wispiest of associations.

Could that something be the fact that the crisis basically affected people who are not big-shot journalists and policymakers?

When we talk about the “borrowers” ruined by the mortgage crisis, we’re not talking about people with prime, fixed-rate loans. That’s Main Street, Mayberry, where delinquencies jumped but stayed in the single digits. We’re talking about subprime, where serious delinquencies topped 40 percent. And when we talk about subprime, we really mean black and Hispanic homeowners, who held those loans in vastly disproportionate numbers.

Obviously, the story of the crisis transcends race—whites on the whole were more likely to undergo foreclosure than blacks—but there’s also no disentangling race from the broader trends. It is a matter of historical record that some minority communities were excluded from the credit system for decades through the policy known as redlining. And that, beginning in the ’90s, those same communities were inundated by a new, heavily marketed, risky mortgage product, which we would later come to know as subprime. Activists accurately called this process “reverse redlining” and (even more accurately) “predatory lending.” The new industry peddling those loans collapsed in scandal by the end of the decade, but as part of its legacy, foreclosures would spike in the inner cities of Cleveland, Pittsburgh, and Chicago, leaving more than half the states and a dozen municipalities scrambling to enact regulations. Even Alan Greenspan felt compelled to speak out: “Discrimination is against the interests of business; yet business people too often practice it,” he said. This was the spring of 2000. Then, the deluge.

At the height of the madness, when subprime made up an insane 27 percent of the multitrillion-dollar home-loan market, nearly half of new African American mortgage holders found themselves in one. Black and Latino borrowers with credit scores of more than 660 were more than three times as likely to be in a subprime loan than their white counterparts. One company, BNC Mortgages, had a customer base nearly 50 percent black or Hispanic, and more than 95 percent of the mortgages it issued were subprime. Fun fact: BNC was a wholly owned unit of Lehman Brothers.

Let’s return to Jacob Faber and his finding about those high-earning families finding themselves with subprime loans. Of course it was African American households earning more than $200,000 who were more likely to wind up in subprime than white households earning $32,000. Go ahead and roll that around for a minute. Graphically it looks like this:

Inevitably, the disparities persisted at the other end of the process. Research shows that upper-income blacks and Latinos were more likely to be foreclosed upon than lower-income whites. In predominantly minority neighborhoods, according to a 2011 report from the Center for Responsible Lending, fully 20 percent of all borrowers lost their homes or went into serious delinquency.

The more we peek inside the subprime machinery, the nastier its racial dynamics look. At Wells Fargo, according to the city of Baltimore’s suit against the firm, mortgage brokers introduced into their lexicon such terms as “mud people” and “ghetto loans.” Elizabeth Jacobson, who one year made $700,000 in commissions while working as a subprime loan officer for the bank, explained in an affidavit that prime loan officers, known as “A reps,” were given handsome referral fees for feeding her prime-qualified customers to whom she could give more expensive and more dangerous mortgages—and did so by the hundreds. Prince George’s County, a solidly middle-class black bastion, was jokingly referred to around the company water cooler as the “subprime capital of Maryland.”

I’m not suggesting that Wall Street has gotten a free pass on its role in the crisis. People get it, sort of. But we also grade on a curve that assumes that banks’ criminal or quasi-criminal conspiracies are par for the course, Wall Street just being Wall Street. In real life, Jordan Belfort, the inspiration for Leonardo DiCaprio’s guilty-pleasure, Oscar-nominated performance in The Wolf of Wall Street, was a possible sociopath who bamboozled retirees with the bad luck to pick up the phone when his boiler-room operation cold-called. But, hey, at least he had style. Borrowers who wound up underwater, by contrast, are pitiful at best. Either they were greedy or dumb or both. They really should have been more careful.

Sorry, everybody was not to blame. “We” didn’t all do it. “Main Street” didn’t succumb to a new tulip mania, and cheap credit didn’t expose anything but the corruption and immorality of a financial industry that systematically put huge numbers of even credit-worthy borrowers into defective products. Cultural theorizing—especially the evidence-free kind—should be seen for what it is: an exercise in complacency. It’s easy. And it’s what you lean on when you don’t want to take on structural problems, the kind you actually have to do something about.



Dean Starkman, a reporting fellow with The Investigative Fund at the Nation Institute and an editor at the Columbia Journalism Review, is the author of The WatchDog that Didn’t Bark: The Financial Crisis and the Disappearance of Investigative Journalism. Research assistance by Jed Bickman.