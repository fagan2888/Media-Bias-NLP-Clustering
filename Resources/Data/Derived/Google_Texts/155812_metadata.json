{"article_title": "How Algorithms Can Beat Back Prejudice in Companies and Courts", "article_keywords": ["foundation", "based", "risk", "defendants", "beat", "data", "human", "companies", "recruiters", "hiring", "prejudice", "courts", "algorithms", "decisions"], "article_url": "http://www.thefiscaltimes.com/2015/07/01/How-Algorithms-Can-Beat-Back-Prejudice-Companies-and-Courts", "article_text": "The way the justice system sets bail for defendants is far from a hard science, and the numbers bear that out: African Americans between the ages of 18 and 29, for example, are given much higher bail amounts than any other demographic, a 2012 study found.\n\nRacism, conscious or not, also exists in the labor market. In a 2009 study published in the American Sociological Review, white, black and Latino job applicants were given equivalent resumes to apply for hundreds of entry-level jobs in New York City. The results found that black applicants were half as likely as equally qualified whites to be asked for a callback or given a job offer.\n\nIn an effort to combat these often undetected biases and predispositions, both the criminal justice system and companies looking to hire are turning more and more to technology.\n\nRelated: The One Profession Robots Haven\u2019t Cracked Yet\n\nAfter two years of testing, the Laura and John Arnold Foundation is introducing a new tool called the Public Safety Assessment to help judges make bail decisions. The foundation said the tool, built by analyzing data from more than 1.5 million cases across 300 U.S. jurisdictions, assesses a defendant based on factors related to criminal history, current charge and current age. Its algorithm gives defendants two scores \u2014 one for the probability of them committing a crime, especially a violent one; the other for the risk that they will skip court dates. In pilot jurisdictions, the algorithm was shown to ignore both race and gender.\n\nSince many pretrial release decisions are made subjectively without risk assessment, \u201cthe result, although unintended, is that many of the individuals who are held in jail before trial pose little risk to public safety, while many violent, high-risk defendants are released into the community,\u201d the foundation said in announcing the new assessment.\n\nThe foundation says its tool, developed at a cost of $1.2 million, can instead provide \u201creliable, predictive information about the risk that a defendant released before trial will engage in violence, commit a new crime, or fail to return to court.\u201d The assessment will be rolled out to 21 jurisdictions, including Arizona, Kentucky and New Jersey as well as the cities of Charlotte, Chicago and Phoenix, the foundation announced last week.\n\nThe idea of incorporating scientific measures of risk into bail decisions carries some obvious appeal, and some lawyers and law enforcement groups have supported the use of such tools, The New York Times reported. But only about 10 percent of courts now use such \u201cevidence-based risk-assessment instruments\u201d when deciding whether to release or detain defendants, according to the Arnold Foundation.\n\nCosts are one reason for that limited adoption, though the foundation hopes to address that issue; it plans to make its Public Safety Assessment available for free to any city, county or state \u201cwithin the next few years,\u201d according to a statement issued last week.\n\nRelated: When Big Data Becomes Big Brother\n\nBias and preconceptions also plague hiring. People usually make unconscious hiring decisions based on factors that don\u2019t have anything to do with the job.\n\nNew startups \u2014 including Textio, Doxa, Entelo, GapJumpers and Gild \u2014 are refining ways to automate hiring. The companies claim that their software will result in a more effective and efficient hiring process. Using computers to filter through candidates\u2019 data could allow recruiters to quickly reach people who are good matches for the position, saving the effort of sifting through profiles of both qualified and unqualified candidates.\n\nAnother benefit of the software could be a more diverse workplace, since the human biases of recruiters, which usually revolve around demographic characteristics such as age, race and gender, are largely eliminated.\n\nTextio CEO Kieran Snyder said in an email that human recruiters are still necessary since there are a number of duties that an algorithm cannot handle, such as deciding what roles to hire for or conducting face-to-face interviews. The software exists to support the humans, she said.\n\n\u201cTextio looks at previous job listings and how they've performed in the past, and then optimizes new listings right as you're typing them so that you attract more qualified and diverse people to apply,\u201d Snyder said. \u201cTextio uses statistics and data to help you get the candidates you want \u2014 so we address bias statistically, on the basis of how previous listings have performed.\u201d\n\nThe idea of hiring people based off an algorithm is controversial, but it\u2019s taking off at some recruiting firms. Well-known headhunters such as Korn Ferry are now using hiring formulas. In August of last year, Korn Ferry introduced a new system, called KF4D, to identify the profile of a perfect candidate for a position.\n\n\u201cAlgorithms don\u2019t replace human recruiters; they replace human error,\u201d Jonathan Foley, vice-president for science at Gild, said in an email. \u201cWhile recruiters rely on certain (occasionally random) benchmarks to assess candidates \u2014 schools, past companies, etc. \u2014 Gild\u2019s algorithms take into account all of the publicly available data about a person, including their skill set, educational and work history. They aren\u2019t affected by biased assumptions about certain schools, companies, or backgrounds. Run our search algorithms and you\u2019ll find a diverse range of candidate recommendations, based purely on skill level, not human prejudice.\"\n\nTop Reads from The Fiscal Times:", "article_metadata": {"og": {"site_name": "The Fiscal Times", "description": "The way the justice system sets bail for defendants is far from a hard science, and th", "title": "How Algorithms Can Beat Back Prejudice in Companies and Courts", "url": "http://www.thefiscaltimes.com/2015/07/01/How-Algorithms-Can-Beat-Back-Prejudice-Companies-and-Courts", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/02172012_Binary_Computers_article.jpg", "type": "article"}, "twitter": {"url": "http://www.thefiscaltimes.com/2015/07/01/How-Algorithms-Can-Beat-Back-Prejudice-Companies-and-Courts", "image": "http://cdn.thefiscaltimes.com/sites/default/files/articles/02172012_Binary_Computers_article.jpg", "creator": "@milliedent", "card": "summary", "title": "The Fiscal Times"}, "viewport": "initial-scale=1, maximum-scale=1", "description": "The way the justice system sets bail for defendants is far from a hard science, and th", "generator": "Drupal 7 (http://drupal.org)"}, "_id": "\"57477af46914bd0286fe0dc6\"", "article_summary": "\u201cAlgorithms don\u2019t replace human recruiters; they replace human error,\u201d Jonathan Foley, vice-president for science at Gild, said in an email.\nPeople usually make unconscious hiring decisions based on factors that don\u2019t have anything to do with the job.\nRun our search algorithms and you\u2019ll find a diverse range of candidate recommendations, based purely on skill level, not human prejudice.\"\n\u2014 Gild\u2019s algorithms take into account all of the publicly available data about a person, including their skill set, educational and work history.\nThe companies claim that their software will result in a more effective and efficient hiring process."}