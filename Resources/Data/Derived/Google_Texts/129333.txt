Why, in the case of the Affordable Care Act, do we force journalists and ordinary Americans to play an utterly unnecessary game of “guess how many beans are in the jar?" The result wastes time and resources, diverting our attention from more substantive issues such as, “How is the law actually affecting the health and the pocketbooks of Americans?”

In late October, Kevin Quealey and Margot Sanger-Katz of the New York Times wrote a fine, visually appealing piece on the change in health insurance coverage. The authors stated that overall enrollment increased by around 10 million people during the previous year, and their article featured maps depicting coverage down to the county level, along with national numbers on income, ethnicity, gender, rural/urban and so forth. All good stuff.



[SEE: Editorial Cartoons on Obamacare]

The problem is that the article was based on data from privately commissioned surveys, laden with the biases and inaccuracies that are unavoidable in polling methodology. In this case, the primary sources of information were Enroll America and Civis Analytics, both of which are strongly associated with the pro-Affordable Care Act camp. That said, cautious skepticism would be advisable for a survey conducted by an anti-Affordable Care Act organization. By its very nature, polling introduces errors.

The phrasing of questions matters a great deal. Individuals surveyed may answer a pollster’s question dishonestly for a variety of reasons, or they may answer incorrectly because they don’t actually know whether they have qualifying insurance coverage. Maybe they mistakenly believe their spouses completed the process – or maybe their applications vanished somewhere in the computers. Even if the question is crystal clear, and every respondent answers honestly and correctly, statistical sampling errors are always present.



[READ: Obamacare Extortion]

My question – and it really is a question – is why we must rely at all on consumer survey data. The number of health insurers in the United States is in the dozens or, at most, in the low hundreds. Suppose the federal government sent each of them a letter reading, "By the 15th day of each month, please send the Department of Health and Human Services a list of names and Social Security numbers of all individuals who had Affordable Care Act-compliant health insurance coverage through your company on the last day of the previous month." By including Social Security numbers, the Health and Human Services Department could avoid problems such as double-counting people who have coverage through more than one carrier. (If you’d prefer that the department not receive the names and Social Security numbers, the lists could be encrypted to provide the information without the individuals being traceable.)

Add a few other variables (ZIP code, age, etc.) and we can largely dispense with opinion polls, focusing instead on hard, objective data. Then, journalists and voters can escape the parlor game of “guess how many are insured” and look at who is actually insured, who is getting care, how much good that care is doing, and how much Americans are paying for insurance and care.



[READ: An Obamacare Catch-22]

In early January, Sanger-Katz referred on Twitter to a Gallup poll claiming that the percentage of Americans without insurance declined in the last quarter of 2014 from 13.4 percent to 12.9 percent. She asked readers for theories to explain this drop. One possibility is simply a sampling error where the pollsters accidentally selected an atypical group of people to ask. The poll only professes accuracy to within 1 percent, and the supposed drop here was only half that amount. Alternatively, the heavy year-end Obamacare news coverage may have biased the respondents’ answers in subtle ways. For example, maybe they incorrectly think their spouses completed the purchases. Maybe they’re unaware that their purchases did not successfully make it through the computer systems. (There may also be deeper problems with the Gallup survey, but we’ll ignore those here.)